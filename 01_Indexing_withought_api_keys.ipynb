{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.12/site-packages (0.3.16)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement python-magic-bin (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for python-magic-bin\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community python-magic-bin tiktoken langchain-openai libmagic langchainhub chromadb langchain unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"True\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/theo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/theo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/theo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = r'data'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.directory.DirectoryLoader at 0x21f21445650>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "        str(data_dir),\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True\n",
    "    )\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  8.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\bedrock_or_sagemaker.md'}, page_content=\"AWS Decision guide\\n\\nAmazon Bedrock or Amazon SageMaker AI?\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon Bedrock or Amazon SageMaker AI?: AWS Decision guide\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.\\n\\nTable of Contents\\n\\nDecision guide.................................................................................................................................. 1 Introduction................................................................................................................................................... 1 Differences...................................................................................................................................................... 4 Use................................................................................................................................................................... 9 Document history.......................................................................................................................... 12\\n\\niii\\n\\nAmazon Bedrock or Amazon SageMaker AI?\\n\\nUnderstand the differences and pick the one that's right for you\\n\\nPurpose Understand the differences between Amazon Bedrock and Amazon SageMaker AI, and determine which service is the best fit for your needs. Last updated August 21, 2024 Covered services • Amazon Bedrock - Amazon SageMaker AI\\n\\nIntroduction\\n\\nAmazon Web Services (AWS) offers a suite of services to help you build machine learning (ML) and generative AI applications. It’s helpful to understand how these services work together to form a generative AI stack, including:\\n\\nGenerative AI-powered services such as Amazon Q, which leverages large language models (LLMs) and other foundation models (FMs).\\n\\nTools for building applications with LLMs and other FMs, including Amazon Bedrock.\\n\\nInfrastructure for model training and inference, such as Amazon SageMaker AI and specialized hardware.\\n\\nIntroduction 1\\n\\nWhen considering which generative AI services you want to use, two services are often considered alongside one another:\\n\\nAmazon Bedrock\\n\\nChoose Amazon Bedrock if you primarily need to use pre-trained foundation models for inference, and want to select the foundation model that best fits your use case. Amazon Bedrock is a fully managed service for building generative AI applications with support for popular foundation models, including Anthropic Claude, Cohere Command & Embed, AI21 Labs Jurassic, Meta Llama, Mistral AI, Stable Diffusion XL and Amazon Titan. Supported FMs are updated on a regular basis.\\n\\nUse Amazon Bedrock to build generative AI applications with security, privacy, and responsible AI —regardless of the foundation model you choose. Amazon Bedrock offers model-independent, single API access, so you can use different foundation models, and upgrade to the latest model versions, with minimal code changes. Amazon Bedrock also supports model fine-tuning and the import of custom models.\\n\\nUse Amazon Bedrock Studio (in preview), which is a new SSO-enabled web interface that your developers can use to work with large language models (LLMs) and other foundation models (FMs), collaborate on projects, and iterate on generative AI applications.\\n\\nIntroduction 2\\n\\nAmazon SageMaker AI\\n\\nAmazon SageMaker AI is a fully managed service designed to help you build, train, and deploy machine learning models at scale. This includes building FMs from scratch, using tools like notebooks, debuggers, profilers, pipelines, and MLOps. Consider SageMaker AI when you have use cases that can benefit from extensive training, fine-tuning, and customization of foundation models. It can also help you through the potentially challenging task of evaluating which FM is the best fit for your use case.\\n\\nUse SageMaker AI’s integrated development environment (IDE) to build, train, and deploy FMs. SageMaker AI offers access to hundreds of pretrained models, including publicly available FMs.\\n\\nFor more information about how Amazon Bedrock and SageMaker AI fit into Amazon’s generative AI services and solutions, see the generative AI decision guide.\\n\\nWhile both Amazon Bedrock and Amazon SageMaker AI enable the development of ML and generative AI applications, they serve different purposes. This guide will help you understand which of these services is the best fit for your needs, including scenarios in which both services can be used together to build generative AI applications.\\n\\nHere's a high-level view of the key differences between these services to get you started.\\n\\nCategory Amazon Bedrock Amazon SageMaker AI Use Cases Ideal for integration of AI capabilities into applications without investing heavily in custom model development Optimized for unique or specialized AI/ML needs that may require custom models Target Users Optimized for developers and businesses without deep machine learning expertise Optimized for data scientists, machine learning engineers, and developers Introduction 3\\n\\nCategory Amazon Bedrock Amazon SageMaker AI Customization You'll primarily use pre-train ed models, but can fine-tune as needed You have full control, and can customize or create models according to your needs Pricing Pay-as-you-go pricing based on the number of API calls made to the service Charges based on the usage of compute resources, storage, and other services Integration Integrate pre-trained models into applications through API calls Integrate custom models into applications, with more customization options Expertise Required Basic level of machine learning expertise needed to use pre-trained models Working knowledge of data science and machine learning skills are helpful for building and optimizing models\\n\\nDifferences between Amazon Bedrock and SageMaker AI\\n\\nLet's examine and compare the capabilities of Amazon Bedrock and Amazon SageMaker AI.\\n\\nUse cases\\n\\nAmazon Bedrock and Amazon SageMaker AI address different use cases based on your specific requirements and resources. Amazon Bedrock - Amazon Bedrock is designed for use cases where you want to efficiently incorporate AI capabilities into your applications without investing heavily in custom model development. For example, a content moderation system for a social media platform could use Amazon Bedrock's pre-trained models to automatically identify and flag inappropriate text or images. Similarly, a customer support chatbot could use Amazon Bedrock's natural language\\n\\nDifferences 4\\n\\nprocessing capabilities to understand and respond to user inquiries. Amazon Bedrock is particularly useful if you have limited machine learning expertise or resources, as it helps you to benefit from AI without the need for extensive in-house development. Amazon SageMaker AI - SageMaker AI is a good choice for unique or specialized AI/ML needs that require custom- built models. It is ideal for scenarios where off-the-shelf solutions are not sufficient, and you have a need for fine-grained control over the model architecture, training process, and deployment. One example of a scenario that would benefit from using SageMaker AI would be a healthcare company developing a model to predict patient outcomes based on specific biomarkers. Another example would be a financial institution creating a fraud detection system tailored to their unique data and risk factors. Additionally, SageMaker AI is suitable for research and development purposes, where data scientists and machine learning engineers can experiment with different algorithms, hyperparameters, and model architectures.\\n\\nTarget users\\n\\nAmazon Bedrock and Amazon SageMaker AI support different targeted users based on their level of expertise and knowledge of machine learning and artificial intelligence. Amazon Bedrock - Amazon Bedrock offers a more accessible and straightforward way to integrate AI functionality into your projects. It’s appropriate for a broad audience, which includes developers and businesses, that has limited experience in building and training machine learning models, but wants to use AI to enhance their applications or workflows.\\n\\nAmazon SageMaker AI - SageMaker AI is predominantly for data scientists, machine learning engineers, and developers who possess the necessary skills and knowledge to build, train, and deploy custom machine learning models. Use SageMaker AI if you are well-versed in data science and machine learning concepts, and require a platform that provides you with the tools and flexibility to create models tailored to your specific needs.\\n\\nDifferences 5\\n\\nCustomization\\n\\nAmazon Bedrock and Amazon SageMaker AI offer different levels of customization capabilities that you can tailor to your specific needs and expertise. Amazon Bedrock - Amazon Bedrock provides pre-trained AI models that you can integrate into applications, with limited customization. You have access to a set of API calls that you use to enter data and receive predictions from these pre-trained models. While this approach drastically simplifies the process of incorporating AI capabilities into applications, it also means that you have less control over the underlying models, unless you customize a model, or import a custom model. Amazon Bedrock's pre-trained models are optimized for common AI tasks and are designed to work well for a wide range of use cases, but they may not be suitable for highly specialized or niche requirements.\\n\\nAmazon Bedrock supports fine-tuning for foundation models (FMs), such as Cohere Command R, Meta Llama 2, Amazon Titan Text Lite, Amazon Titan Text Express, Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can now fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. - Customize models for specific tasks and use cases, including FM fine-tuning and pre-training. Bring your own customized model with custom model import (in preview).\\n\\nAmazon SageMaker AI - Amazon SageMaker AI provides extensive customization options, giving you full control over the entire machine learning workflow. With SageMaker AI, you can fine-tune every aspect of your models, from data preprocessing and feature engineering to model architecture and hyperparameter optimization. By using this level of customization, you can create highly specialized models that are tailored to your unique business requirements. SageMaker AI supports a wide range of popular machine learning frameworks, such as TensorFlow, PyTorch, and Apache MXNet, allowing you to use your preferred tools and libraries for building and training models. - Use Amazon SageMaker AI JumpStart to evaluate, compare, and select FMs based on pre- defined quality and responsibility.\\n\\nDifferences 6\\n\\nChoose which FM to use with Amazon SageMaker AI Clarify. Use SageMaker AI Clarify to create model evaluation jobs, that you use to evaluate and compare model quality and responsibility metrics for text-based foundation models from JumpStart.\\n\\nGenerate predictions using Amazon SageMaker AI Canvas, without needing to write any code. Use SageMaker AI Canvas in collaboration with Amazon Bedrock to fine-tune and deploy language models. This blog post describes how you can use them to optimize customer interaction by working with your own datasets, such as your product FAQs, in Amazon Bedrock and Amazon SageMaker AI JumpStart.\\n\\nPricing\\n\\nAmazon Bedrock and Amazon SageMaker AI have different pricing models that reflect their target users and the services they provide. Amazon Bedrock - Amazon Bedrock employs a simple pricing model based on the number of API calls made to the service. You pay a fixed price per API call, which includes the cost of running the pre- trained models and any associated data processing. This straightforward pricing structure makes it more efficient for you to estimate and control your costs, as you pay only for the actual usage of the service. Amazon Bedrock's pricing model is particularly well-suited for applications with predictable workloads, or for cases where you want more transparency in your AI-related expenses.\\n\\nAmazon SageMaker AI - SageMaker AI follows a pay-as-you-go pricing model based on the usage of compute resources, storage, and other services consumed during the machine learning process. You’re charged for the instances that you use to build, train, and deploy you models, with prices varying depending on the instance type and size. Additionally, you incur costs for data storage, data transfer, and other associated services like data labeling and model monitoring. This pricing model provides flexibility and allows you to optimize costs based on your specific requirements. However, it also means that costs can vary and may require careful management, especially for resource-intensive projects.\\n\\nDifferences 7\\n\\nIntegration\\n\\nAmazon Bedrock and Amazon SageMaker AI offer different approaches to integrating machine learning models into applications, catering to your specific needs and expertise. Amazon Bedrock - Amazon Bedrock simplifies the integration process by providing pre-trained models that you can access directly through API calls. Use the Amazon Bedrock SDK or REST API to send input data and receive predictions from the models without needing to manage the underlying infrastructure. This approach significantly reduces the complexity and time required to integrate AI capabilities into applications, making it more accessible to developers with limited machine learning expertise. However, this ease of integration comes at the cost of limited customization options, as you’re restricted to the pre-trained models and APIs provided by Amazon Bedrock.\\n\\nAmazon SageMaker AI - SageMaker AI provides a comprehensive platform for building, training, and deploying custom machine learning models. However, integrating these models into applications requires more effort and technical expertise compared to Amazon Bedrock. You need to use the SageMaker AI SDK or API to access the trained models and build the necessary infrastructure to expose them as endpoints. This process involves creating and configuring API Gateway, Lambda functions, and other AWS services to enable communication between the application and the deployed model. While SageMaker AI provides tools and templates to simplify this process, it still requires a deeper understanding of AWS services and machine learning model deployment.\\n\\nExpertise required\\n\\nAmazon Bedrock and Amazon SageMaker AI are optimized for different levels of machine learning expertise. Amazon Bedrock - Amazon Bedrock is more accessible to a broader range of users, including developers and businesses with limited machine learning expertise. By providing pre-trained models that can be easily integrated into applications through API calls, Amazon Bedrock abstracts away much of the complexity associated with building and deploying machine learning\\n\\nDifferences 8\\n\\nmodels. You don't need to worry about data preprocessing, model selection, or infrastructure management, as these aspects are handled by the Amazon Bedrock service. This allows you to focus on integrating AI capabilities into your applications without needing to invest significant time and resources in acquiring deep machine learning knowledge. Amazon SageMaker AI - If you have deeper expertise in data science and machine learning, SageMaker AI provides a powerful and flexible platform for building, training, and deploying custom models. While SageMaker AI aims to simplify the machine learning workflow, it still requires a significant level of technical expertise to take full advantage of its capabilities. You’ll benefit from being proficient in programming languages like Python, along with a deep understanding of machine learning concepts, such as data preprocessing, model selection, and hyperparameter tuning. Additionally, you should be comfortable working with various AWS services and managing the infrastructure required to deploy and integrate their models. As a result, SageMaker AI may have a steeper learning curve if you’re new to machine learning or have limited experience with AWS.\\n\\nThe choice between Amazon Bedrock and Amazon SageMaker AI is not always mutually exclusive. In some cases, you may benefit from using both services together. For example, you could use Amazon Bedrock to quickly prototype and deploy a foundation model, and then use SageMaker AI to further refine and optimize the model for better performance. For example, this blog post describes how you can use Amazon Bedrock and Amazon SageMaker AI together to optimize customer interaction by working with your own datasets (such as your product FAQs.\\n\\nUltimately, the decision between Amazon Bedrock and Amazon SageMaker AI depends on your specific requirements. Evaluating these factors can help you make an informed decision and choose the service that is most suitable for your needs.\\n\\nFor more information about Amazon’s generative AI services and solutions, see the generative AI decision guide.\\n\\nUse\\n\\nNow that you've read about the criteria for choosing between Amazon Bedrock and Amazon SageMaker AI, you can select the service that meets your needs, and use the following information to help you get started using each of them.\\n\\nUse 9\\n\\nAmazon Bedrock\\n\\nWhat is Amazon Bedrock?\\n\\nDescribes how to use this fully managed service to make foundation models (FMs) from Amazon and third parties available for your use through a unified API. Explore the guide - Frequently asked questions about Amazon Bedrock\\n\\nGet answers to the most commonly-asked questions about Amazon Bedrock. These include how to use agents, security considerations, details about Amazon Bedrock software development kits (SDKs), retrieval augmented generation, how to use model evaluation, and billing. Read the FAQs - Guidance for generating product descriptions with Amazon Bedrock\\n\\nDescribes how to use Amazon Bedrock in your solution to automate your product review and approval process for an e-commerce marketplace or retail website. Explore the solution Amazon Bedrock Studio\\n\\nWhat is Amazon Bedrock Studio?\\n\\nDescribes how to use this web app to prototype apps that use Amazon Bedrock models and features, without having to set up and use a developer environment. Explore the guide - Build generative AI applications with Amazon Bedrock Studio\\n\\nThis blog post describes how you can build applications using a wide array of top performing models. It then explains how to evaluate and share your generative AI apps within Amazon Bedrock Studio. Use 10\\n\\nRead the blog - Building an app with Amazon Bedrock Studio\\n\\nUse the Build mode in Amazon Bedrock Studio to create prototype apps that uses Amazon Bedrock models and features. You can also use the Build mode to try experiments not supported in the Explore mode playground, such as setting inference parameters. Explore the guide Amazon SageMaker AI\\n\\nWhat is Amazon SageMaker AI?\\n\\nDescribes how you can use this fully managed machine learning (ML) service to build, train, and deploy ML models into a production-ready hosted environment. Explore the guide - Get started with Amazon SageMaker AI\\n\\nDescribes how to join an Amazon SageMaker AI domain, giving you access to Amazon SageMaker AI Studio and RStudio on SageMaker AI. Explore the guide - Get started with Amazon SageMaker AI JumpStart\\n\\nExplore SageMaker AI JumpStart solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI. Explore the guide Use 11\\n\\nDocument history\\n\\nThe following table describes the important changes to this decision guide. For notifications about updates to this guide, you can subscribe to an RSS feed.\\n\\nChange Description Date Minor updates Minor updates to improve readability. August 21, 2024 Minor updates Minor updates to reflect the latest Amazon Bedrock and Amazon SageMaker AI features. July 22, 2024 Initial release Initial release of the decision guide. July 11, 2024 12\"),\n",
       " Document(metadata={'source': 'data\\\\containers-on-aws-how-to-choose.md'}, page_content=\"AWS Decision Guide\\n\\nChoosing an AWS container service\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nChoosing an AWS container service: AWS Decision Guide\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.\\n\\nTable of Contents\\n\\nDecision guide..................................................................................................................................\\n\\nIntroduction...................................................................................................................................................\\n\\nUnderstand.....................................................................................................................................................\\n\\nConsider..........................................................................................................................................................\\n\\nChoose.............................................................................................................................................................\\n\\nUse...................................................................................................................................................................\\n\\nCapacity.....................................................................................................................................................\\n\\nOrchestration.........................................................................................................................................\\n\\nVertical solutions..................................................................................................................................\\n\\nTools and services with container support......................................................................................\\n\\nOn-premises...........................................................................................................................................\\n\\nExplore..........................................................................................................................................................\\n\\nDocument history..........................................................................................................................\\n\\nChoosing an AWS container service\\n\\nTaking the first step\\n\\nPurpose Determine which AWS container service is the best fit for your organization. Last updated April 5, 2024 Covered services • Amazon EC - Amazon ECR - Amazon ECS - Amazon EKS - Amazon Lightsail - AWS App Runner - AWS Batch - AWS Copilot - AWS Fargate - AWS Lambda - AWS Outposts - Red Hat OpenShift Service on AWS (ROSA)\\n\\nIntroduction...................................................................................................................................................\\n\\nContainers are a key component of modern application development. They are the standard for organizing compute resources, and managing the content of your application deployments.\\n\\nContainers provide a discrete reproducible compute environment for building software to deploy in the cloud. They also simplify packaging and dependency management. You can use them for everything from orchestrating web applications or very large multi-cluster estates to testing your work and doing a proof of concept on your laptop.\\n\\nThis decision guide helps you get started and choose the right AWS container service for your modern application development.\\n\\nIntroduction 1\\n\\nThis 3½-minute excerpt is from an 11-minute presentation at re:Invent 2023 by Umar Saeed, an AWS senior manager and solutions architect. He provides a quick overview of AWS container choices.\\n\\nUnderstand.....................................................................................................................................................\\n\\nContainers offer a number of advantages for packaging, deploying, and running applications:\\n\\nPortability: Benefit from a consistent runtime environment that can run on any platform that supports the container runtime.\\n\\nScalability: Scale applications up or down, based on demand, with lightweight and easily replicated containers.\\n\\nConsistency: Ensure that the application runs the same way in all environments with a consistent runtime environment.\\n\\nEfficiency: Use fewer resources than traditional virtual machines with lightweight containers.\\n\\nIsolation: Improve security and reliability with containers' process-level isolation, with which applications running in separate containers cannot interfere with each other, improving security and reliability.\\n\\nAgility: Reduce the time that it takes to bring new features or applications to market by quickly packaging and deploying applications.\\n\\nUnderstand 2\\n\\nYou can think about the universe of AWS container services in three distinct layers:\\n\\nThe Compute capacity layer is where your containers actually run. This layer consists of:\\n\\nAmazon Elastic Compute Cloud (Amazon EC2) instances: These instances provide the underlying compute capacity for running containers. You can choose from a wide range of instance types and sizes to match your application requirements. EC2 instances can be used as the compute layer for both Amazon ECS and Amazon EKS.\\n\\nAWS Fargate: Fargate is a serverless compute engine for containers with which you can run containers without managing the underlying infrastructure. It removes the need to provision and manage EC2 instances. You can use Fargate with both Amazon ECS and Amazon EKS.\\n\\nAWS Outposts: AWS Outposts is a fully managed service that extends AWS infrastructure and services to your on-premises or hybrid environment. With AWS Outposts, you can run containers on AWS infrastructure deployed in your own data center.\\n\\nThe orchestration layer schedules and scales your environment. This layer includes:\\n\\nAmazon Elastic Container Service (Amazon ECS): Amazon ECS is a fully managed container orchestration service that simplifies the deployment, management, and scaling of containerized applications. It supports Docker containers. You can use Amazon ECS to define tasks and services, handle service discovery, and manage the lifecycle of containers.\\n\\nUnderstand 3\\n\\nAmazon Elastic Kubernetes Service (Amazon EKS): Amazon EKS is a managed Kubernetes service with which you can deploy, manage, and scale containerized applications using Kubernetes. It provides a highly available and secure Kubernetes control plane.\\n\\nRed Hat OpenShift Service on AWS (ROSA): ROSA is a fully managed service with which you can deploy and run Red Hat OpenShift clusters on AWS infrastructure. OpenShift is a popular enterprise-grade Kubernetes platform that extends the capabilities of Kubernetes with additional features and tools for building, deploying, and managing containerized applications.\\n\\nThe Vertical solutions layer is a set of vertical integration services that provide higher-level and bundled services that simplify the process of deploying and managing applications. The AWS services in this layer are:\\n\\nAWS App Runner: AWS App Runner is a fully managed service designed to simplify the deployment and use of containerized web applications and APIs. You provide your container image, and App Runner automatically builds, deploys, and scales your application. It handles the provisioning of the underlying compute resources, load balancing, and automatic scaling based on incoming traffic.\\n\\nAmazon Lightsail: Amazon Lightsail is a cloud platform that offers pre-configured virtual private servers (instances) and other resources for running applications. It provides pre-defined configurations for quickly provisioning compute instances, databases, storage, and networking resources. Lightsail supports running containerized applications by provisioning instances with Docker pre-installed, aimed at easing the deployment and management of your containers.\\n\\nAWS Batch: AWS Batch is a fully managed service with which you can run batch computing workloads on AWS. It dynamically provisions the optimal compute resources based on the volume and specific resource requirements of the batch jobs that you submit. It automatically handles job scheduling, resource provisioning, and scaling based on the workload requirements.\\n\\nAmazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker container registry with which you can store, manage, and deploy Docker container images. It is designed to provide secure and scalable storage for your container images and simplify provisioning containers with the desired images.\\n\\nNote AWS provides a variety of ways to deploy and run containers. One of the first considerations is your preference for either a serverless operational model or a Kubernetes operation model. In practice, most customers use both to varying degrees. Understand 4\\n\\nThe choice of operating model is explored in-depth in the Choosing a modern application strategy decision guide, which is a useful resource for anyone who wants to explore this question further. In addition, the Containers and Serverless Recommendation Guide takes you through the choices to make when choosing your operating model.\\n\\nConsider..........................................................................................................................................................\\n\\nIt's important to choose a container service that aligns to your application requirements and operational preferences. The following section outlines some of the key criteria to consider when choosing a container service, as well as supporting tools and services.\\n\\nManaged service and operation overhead\\n\\nBuilding with containers on AWS uses services with higher levels of abstraction to shift the operational overhead of maintaining infrastructure to AWS. Organizations may choose the cloud to reduce operational cost by using standardized managed services with higher levels of abstraction so that developers and operators can focus on their unique activities that add value, instead of on undifferentiated tasks. Workload characteristics\\n\\nUnderstanding your workload patterns can help you make architecture choices. Workload patterns can include web applications, API-based microservices, event-driven applications, streaming and messaging, data pipelines, IT automations, and more. Some workloads perform better or are more cost effective in one compute environment versus another type. Application portability\\n\\nMany customers want to ensure that their applications can run in—and be migrated or moved to—a different environment. It's important for them to be able to preserve choice, or run an application both on premises and in the cloud. We recommend building competency in software architectures and build packaging with which you can readily port differentiating business logic between compute services. Applications built using some technologies might run more effectively on some compute services rather than others. Consider 5\\n\\nOrganization size and skills\\n\\nThe skills of your organization are a major factor when deciding which container services you use. The approach you take can require some investment in DevOps and Site Reliability Engineer (SRE) teams. Building out an automated pipeline to deploy applications is common for most modern application development. Some choices elevate the amount of management you need to do. For example, some organizations have skills and resources to run and manage a Kubernetes implementation, because they invest in strong SRE teams to manage Kubernetes clusters and find value in the associated skill portability. These teams handle frequent cluster upgrades. For example, Kubernetes has three major releases a year, and deprecates old versions. Organization size is a key factor, as smaller organizations might have a more limited IT team made up of people fulfilling multiple roles, while larger enterprises may support hundreds of workloads in production at once. Ease of deployment\\n\\nDifferent AWS container services meet unique needs in terms of deployment complexity. Here's how each service is optimized for its own role: - AWS App Runner offers the most straightforward path for you to deploy your application on the internet without managing or customizing the underlying infrastructure. - Amazon ECS is a good choice if you need more control over the network and security configurations without sacrificing scale or features. - Amazon EKS provides flexibility and control over application deployment and orchestration provided by Kubernetes technology.\\n\\nChoose.............................................................................................................................................................\\n\\nNow that you know the criteria by which you are evaluating your container options, you are ready to choose which AWS container services might be a good fit for your organizational requirements.\\n\\nThe following table highlights which services are optimized for which circumstances. Use the table to help determine which container services and tools are.\\n\\nChoose 6\\n\\nContainers category When would you use it? Services Capacity Use when you want to run your containers on self-mana ged AWS virtual machines or AWS managed compute. AWS Fargate Amazon EC AWS Outposts Orchestration Use when you need the capacity to deploy and manage up to thousands of containers. Amazon ECS Amazon EKS Red Hat OpenShift Service on AWS Vertical solutions Use when you or your team don't have a lot of experienc e with containers or infrastru cture. AWS App Runner AWS Lambda Amazon Lightsail AWS Batch Tools and services with container support Use for supporting your container operations. AWS Copilot Amazon ECR AWS Cloud Map On-premises Use these services for flexibili ty in where you run your container-based applications. Amazon ECS Anywhere Amazon EKS Anywhere\\n\\nUse...................................................................................................................................................................\\n\\nYou should now have a clear understanding of each AWS container service (and the supporting AWS tools and services) and which one might be the best fit for your organization and use case.\\n\\nUse 7\\n\\nTo explore how to use and learn more about each of the available AWS container services, we have provided a pathway to explore how each of the services work. The following section provides links to in-depth documentation, hands-on tutorials, and resources to get you started.\\n\\nCapacity.....................................................................................................................................................\\n\\nAmazon EC\\n\\nWhat is Amazon EC2? Get an overview of Amazon EC2. This guide not only provides an introduction to the service, but also covers how to get started using it and then provides in-depth descripti ons of key features and how to use them. Explore the guides Amazon EC2 instance types When you launch an EC2 instance, the instance type that you specify determine s the hardware of the host computer used for your instance. Each instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities. This guide walks you through EC2 instance types. Explore the guide Amazon EC2 Auto Scaling with EC2 Spot Instances Learn how to create a stateless, fault-tol erant workload using Amazon EC2 Auto Scaling with launch templates to request Amazon EC2 Spot Instances. Get started with the tutorial Capacity 8\\n\\nAWS Fargate\\n\\nGetting started with AWS Fargate This guide explains the basics of AWS Fargate, a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. Explore the guide Getting started with the console using Linux containers on AWS Fargate Get started with Amazon ECS on AWS Fargate by using the Fargate launch type for your tasks in the Regions where Amazon ECS supports AWS Fargate. Explore the guide Creating a cluster with a Fargate Linux task using the AWS CLI Set up a cluster, register a task definition, run a Linux task, and perform other common scenarios in Amazon ECS with the AWS CLI. Explore the guide AWS Outposts\\n\\nGetting started with AWS Outposts What is AWS Outposts? Capacity 9\\n\\nAccess the complete set of AWS Outposts technical documentation. Explore the guides Get an introduction to this fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. Explore the guide AWS Outposts servers pricing Get details on the pricing of AWS Outposts servers. Get details on pricing\\n\\nOrchestration.........................................................................................................................................\\n\\nAmazon ECS\\n\\nGetting started with Amazon ECS Get an introduction to the tools available to access Amazon ECS and introductory step- by-step procedures to run containers. Explore the guide Tutorials for Amazon ECS Learn how to perform common tasks—inc luding the creation of clusters and VPCs— when using Amazon ECS. Get started with the tutorials Orchestration 10\\n\\nAmazon ECS Workshop Use this workshop to become familiar with AWS Fargate, Amazon ECS, and Docker container workflows. Explore the workshop Deploy Docker containers on Amazon ECS Learn how to run a Docker-enabled sample application on an Amazon ECS cluster behind a load balancer, test the sample application, and delete your resources to avoid charges. Explore the guide Amazon EKS\\n\\nGetting started with Amazon EKS Learn more about Amazon EKS, a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Explore the guide Amazon EKS deployment Explore Amazon EKS deployment options on AWS and learn how to use it to manage a general containerized application. Explore the guide Amazon EKS Quick Start Reference Deployment Using a Quick Start reference deploymen t guide, get step-by-step instructions for deploying Amazon EKS clusters. Explore the guide Amazon EKS workshop Explore practical exercises to learn about Amazon EKS. Visit the workshop Orchestration 11\\n\\nRed Hat OpenShift Service on AWS\\n\\nWhat is Red Hat OpenShift Service on AWS? Learn how to use this managed service to build, scale, and deploy containerized applications with the Red Hat OpenShift enterprise Kubernetes platform on AWS. Explore the guide Getting started with Red Hat OpenShift Service on AWS Learn how to get started using Red Hat OpenShift Service on AWS (ROSA). Explore the guide Why would you use ROSA? Watch a video to learn when to use Red Hat OpenShift over standard Kubernetes and explore ROSA in depth. Watch the video\\n\\nVertical solutions..................................................................................................................................\\n\\nAWS App Runner\\n\\nVertical solutions 12\\n\\nWhat is AWS App Runner? Learn when to use this service to deploy from source code or a container image directly to a scalable and secure web application in the AWS Cloud. Explore the guide Getting started with AWS App Runner Use this tutorial to configure the source code and deployment, the service build, and the service runtime to deploy your application to AWS App Runner. Use the tutorial Deploy a web app using AWS App Runner Follow these step-by-step instructions to deploy a containerized web application using AWS App Runner. Use the tutorial AWS Lambda\\n\\nWhat is AWS Lambda? Learn how to use Lambda to run your code on a high-availability compute infrastru cture and perform all of the administration of the compute resources, including server and operating system maintenance, capacity AWS Lambda documentation Work with AWS Lambda documentation to understand how you can use this service to run code without provisioning or managing servers and only pay for the compute time that you consume. Vertical solutions 13\\n\\nprovisioning and automatic scaling, and logging. Explore the guide Explore the guides Working with Lambda container images locally Learn how you can use a deployment package to deploy your function code to Lambda. Lambda supports two types of deployment packages: container images and .zip file archives. Explore the guide Amazon Lightsail\\n\\nWhat is Amazon Lightsail? Get the full story on Amazon Lightsail, including what it does and how you can benefit from it. This guide also includes step- by-step guidance to help you get started using Lightsail and then configure it to meet your needs. Creating Lightsail container service images Learn how to create a container image on your local machine using a Dockerfile. You can then push it to your Lightsail container service to deploy it. Explore the guide Vertical solutions 14\\n\\nExplore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\\n\\nWhat is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\\n\\nUse these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\\n\\nTools and services with container support......................................................................................\\n\\nAWS Copilot\\n\\nGetting started with Amazon ECS using AWS Copilot Get started with Amazon ECS using AWS Copilot by deploying an Amazon ECS application. Explore the guide AWS Copilot CLI documentation Learn how to use the AWS Copilot CLI, a tool for developers to build, release, and operate production-ready containerized applications on AWS App Runner and Amazon ECS on AWS Fargate. Explore the documentation Introduction to Amazon ECS using AWS Copilot CLI Learn how to deploy your application to Amazon ECS using AWS Copilot. Watch the video Tools and services with container support 16\\n\\nAmazon ECR\\n\\nAmazon ECR documentation Use the Amazon ECR documentation to explore the best ways to use this fully managed container registry. Explore the guides What is Amazon Elastic Container Registry (Amazon ECR)? A guide to getting started with—and using— Amazon ECR. Explore the guide Amazon ECR in Multi-Account and Multi- Region Architectures Explore key considerations for Amazon ECR architectures that span across AWS accounts and AWS Regions, and architectures related to hypothetical customer use cases. Read the blog post AWS Cloud Map\\n\\nTools and services with container support 17\\n\\nAWS Cloud Map documentation Use the AWS Cloud Map developer guide, API reference, and AWS Cloud Map (service discovery) in the AWS CLI Reference to get the most from this service. Explore the guides What is AWS Cloud Map? Learn how you can use AWS Cloud Map to create and maintain a map of backend services and resources for your applications. Explore the guide AWS Cloud Map FAQs Get answers to frequently asked questions about AWS Cloud Map. Explore the FAQs\\n\\nOn-premises...........................................................................................................................................\\n\\nAmazon ECS Anywhere\\n\\nWhat is Amazon ECS Anywhere? Learn how Amazon ECS Anywhere provides support for registering an external instance , such as an on-premises server or virtual machine (VM), to your Amazon ECS cluster. Amazon ECS Anywhere pricing Use this pricing guide to understand Amazon ECS Anywhere pricing, which is based on a model in which you are charged based on the amount of time the instances you have registered to an Amazon ECS cluster are On-premises 18\\n\\nExplore the guide connected to the ECS control plane, rounded up to the nearest second. Explore the pricing guide Amazon ECS Anywhere FAQs Get answers to frequently asked questions about Amazon ECS Anywhere. Explore the FAQs Amazon EKS Anywhere\\n\\nAmazon EKS Anywhere documentation Use the documentation to understand the use of and best practices for Amazon EKS Anywhere. Read the documentation Amazon EKS Anywhere pricing Use this pricing guide to understand Amazon EKS Anywhere pricing. Explore the pricing guide On-premises 19\\n\\nAmazon EKS Anywhere FAQs Get answers to frequently asked questions about Amazon EKS Anywhere. Explore the FAQs\\n\\nExplore..........................................................................................................................................................\\n\\nFor your role - Developers - Solution Architects - Professional development - Startups - Decision makers\\n\\nFor an introduction - Docker - Kubernetes - Breaking a monolith into microservices\\n\\nFor a video - Containers from the couch - How to containerize anything! - Building a container CI/CD pipeline - Building a container app with AWS CDK\\n\\nArchitecture diagrams Explore reference architecture diagrams for containers on AWS. Explore architecture diagrams Whitepapers Explore whitepapers to help you get started and learn best practices. Explore whitepapers AWS solutions Explore vetted solutions and architectural guidance for common use cases for containers. Explore solutions Explore 20\\n\\nDocument history..........................................................................................................................\\n\\nThe following table describes the important changes to this decision guide. For notifications about updates to this guide, you can subscribe to an RSS feed.\\n\\nChange Description Date Guide updated Added AWS Copilot, AWS Batch, and AWS Outposts. Changed capacity, orchestra tion, and provisioning to compute capacity, orchestra tion, and vertical solutions. Numerous editorial changes throughout. April 5, 2024 Initial publication Guide first published. April 26, 2023 21\"),\n",
       " Document(metadata={'source': 'data\\\\generative-ai-on-aws-how-to-choose.md'}, page_content=\"AWS Decision Guide\\n\\nChoosing a generative AI service\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nChoosing a generative AI service: AWS Decision Guide\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.\\n\\nTable of Contents\\n\\nDecision Guide.................................................................................................................................\\n\\nIntroduction...................................................................................................................................................\\n\\nUnderstand.....................................................................................................................................................\\n\\nConsider........................................................................................................................................................\\n\\nChoose..........................................................................................................................................................\\n\\nUse.................................................................................................................................................................\\n\\nExplore..........................................................................................................................................................\\n\\nResources......................................................................................................................................................\\n\\nDocument history..........................................................................................................................\\n\\nChoosing a generative AI service\\n\\nTaking the first step\\n\\nPurpose Determine which AWS generative AI services are the best fit for your organization. Last updated August 28, 2024 Covered services • Amazon Bedrock - Amazon Bedrock Studio - Amazon Q Business - Amazon Q Developer - Amazon SageMaker AI - Amazon Titan foundation models - Public foundation models\\n\\nIntroduction...................................................................................................................................................\\n\\nGenerative AI is a set of artificial intelligence (AI) systems and models designed to generate content such as code, text, images, music, or other forms of data. These systems can produce new content based on patterns and knowledge learned from existing data. Increasingly, organizations and businesses are using generative AI to:\\n\\nAutomate creative workflows — Use generative AI services to automate the workflows of time- consuming creative processes such as writing, image or video creation, and graphic design.\\n\\nCustomize and personalize content — Generate targeted content, product recommendations, and customized offerings for an audience-specific context.\\n\\nAugment data — Synthesize large training datasets for other ML models to unlock scenarios where human-labeled data is scarce.\\n\\nReduce cost — Potentially lower costs by using synthesized data, content, and digital assets.\\n\\nFaster experimentation — Test and iterate on more content variations and creative concepts than would be possible manually.\\n\\nIntroduction 1\\n\\nThis guide helps you select the AWS generative AI services and tools that are the best fit for your needs and your organization.\\n\\nUnderstand.....................................................................................................................................................\\n\\nAmazon offers a range of generative AI services, applications, tools, and supporting infrastructure. Which of these you use depends a lot on the following factors:\\n\\nWhat you’re trying to do\\n\\nHow much choice you need in the foundation models that you use\\n\\nThe degree of customization you need in your generative AI applications\\n\\nThe expertise within your organization\\n\\nAmazon Q — Get pre-defined applications for your use case\\n\\nAt the top of Amazon's generative AI stack, Amazon Q generative AI-based applications use large language models (LLMs) and foundation models. However, they don’t require that you explicitly choose a model. Each of these applications is aimed at a different use case and all are powered by Amazon Bedrock.\\n\\nUnderstand 2\\n\\nLearn more about the primary Amazon Q generative AI–powered assistants currently available:\\n\\nAmazon Q Business\\n\\nAmazon Q Business can answer questions, provide summaries, generate content, and securely complete tasks based on the data in your enterprise systems. It supports the general use case of using generative AI to start making the most of the information in your enterprise. With Amazon Q Business, you can make English-language queries about that information. It provides responses in a manner appropriate to your team’s needs. In addition, you can create lightweight, purpose-built Amazon Q Apps within your Amazon Q Business Pro subscription. Amazon Q Developer\\n\\nWith Amazon Q Developer, you can understand, build, extend, and operate AWS applications. The supported use cases include tasks that range from coding, testing, and upgrading applications, to diagnosing errors, performing security scanning and fixes, and optimizing AWS resources. The advanced, multistep planning and reasoning capabilities in Amazon Q Developer are aimed at reducing the work involved in common tasks (such as performing Java version upgrades). These capabilities can also help implement new features generated from developer requests. Amazon Q Developer is also available as a feature in several other AWS services including AWS Chatbot, Amazon CodeCatalyst, Amazon EC2, AWS Glue, and VPC Reachability Analyzer. Chat with Amazon Q Developer to query and explore your AWS infrastructure directly from the AWS Management Console. Using natural language prompts to interact with your AWS account, you can get specific resource details and ask about relationships between resources. Amazon Q in QuickSight\\n\\nAmazon Q in QuickSight is aimed at meeting the needs of a specific use case: getting actionable insights from your data by connecting Amazon Q to the Amazon Q QuickSight business intelligence (BI) service. You can use it to build visualizations of your data, summarize insights, answer data questions, and build data stories using natural language. Amazon Q in Connect\\n\\nAmazon Q in Connect can automatically detect customer issues. It provides your customer service agents with contextual customer information along with suggested responses and actions for faster resolution of issues. It combines the capabilities of the Amazon Connect cloud contact center service with Amazon Q. Amazon Q in Connect can use your real-time Understand 3\\n\\nconversations with your customers, along with relevant company content, to recommend what to say or what actions an agent should take to assist customers. Amazon Bedrock — Choose your foundation models\\n\\nIf you're developing custom AI applications, need access to multiple foundation models, and want more control over the AI models and outputs, then Amazon Bedrock could be the service that meets your needs. Amazon Bedrock is a fully managed service, and it supports a choice of popular foundation models, including Anthropic Claude, Cohere Command & Embed, AI21 Labs Jurassic, Meta Llama, Mistral AI, Stable Diffusion XL and Amazon Titan.\\n\\nIn addition, Amazon Bedrock provides what you need to build generative AI applications with security, privacy, and responsible AI—regardless of the foundation model you choose. It also offers model-independent, single API access and the flexibility to use different foundation models and upgrade to the latest model versions, with minimal code changes.\\n\\nLearn more about the key features of Amazon Bedrock:\\n\\nModel customization\\n\\nModel customization can deliver differentiated and personalized user experiences. To customize models for specific tasks, you can privately fine-tune FMs using your own labeled datasets. Custom models include capabilities such as fine-tuning and continued pre-training using unlabeled datasets. The list of FMs for which Amazon Bedrock supports fine-tuning includes Cohere Command, Meta Llama 2, Amazon Titan Text Lite and Express, Amazon Titan Multimodal Embeddings, and Amazon Titan Image Generator. You can fine-tune Anthropic Claude 3 Haiku in a preview capacity in the US West (Oregon) AWS Region. The list of supported FMs is updated on an ongoing basis. Understand 4\\n\\nIn addition, you can use Amazon Bedrock Custom Model Import (currently in preview) to bring your own custom models and use them within Amazon Bedrock. Agents\\n\\nAmazon Bedrock Agents helps you plan and create multistep tasks using company systems and data sources—from answering customer questions about your product availability to taking their orders. You can create an agent by first selecting an FM and then providing it access to your enterprise systems, knowledge bases, and AWS Lambda functions to run your APIs securely. An agent analyzes the user request, and a Lambda function or your application can automatically call the necessary APIs and data sources to fulfill the request. Agents can retain memory across multiple interactions to remember where you last left off and provide better recommendations based on prior interactions. Agents can also interpret code to tackle complex data-driven use cases, such as data analysis, data visualization, text processing, solving equations, and optimization problems. Guardrails\\n\\nAmazon Bedrock Guardrails evaluates user inputs and FM responses based on use case specific policies, and provides an additional layer of safeguards, regardless of the underlying FM. Using a short natural language description, you can use Amazon Bedrock Guardrails to define a set of topics to avoid within the context of your application. Guardrails detects and blocks user inputs and FM responses that fall into the restricted topics. Guardrails supports contextual grounding checks, to detect hallucinations in model responses for applications using Retrieval Augmented Generation (RAG) and summarization applications. Contextual grounding checks add to the safety protection in Guardrails to make sure the LLM response is based on the right enterprise source data, and evaluates the LLM response to confirm that it’s relevant to the user’s query or instruction. Contextual grounding checks can detect and filter over 75% hallucinated responses for RAG and summarization workloads. Knowledge Bases\\n\\nAmazon Bedrock Knowledge Bases is a fully managed capability that you can use to implement the entire Retrieval Augmented Generation (RAG) workflow—from ingestion to retrieval and prompt augmentation—without having to build custom integrations to data sources, and manage data flows. Session context management is built in, so your application can support multi-turn conversations. You can use the Retrieve API to fetch relevant results for a user query from knowledge bases. Understand 5\\n\\nWith RAG, you can provide a model with new knowledge or up-to-date info from multiple sources, including document repositories, databases, and APIs. For example, the model might use RAG to retrieve search results from Amazon OpenSearch Service or documents from Amazon Simple Storage Service. Amazon Bedrock Knowledge Bases fully manages this experience by connecting to your private data sources, including Amazon Aurora, Amazon OpenSearch Serverless, MongoDB, Pinecone, and Redis Enterprise Cloud. This list includes connectors for Salesforce, Confluence, and SharePoint (in preview), so you can access more business data to customize models for your specific needs. Converse API\\n\\nUse the Amazon Bedrock Converse API to create conversational applications that send and receive messages to and from an Amazon Bedrock model. For example, you can create a chatbot that maintains a conversation over many turns and uses a persona or tone customization that is unique to your needs, such as a helpful technical support assistant. Tool use (function calling)\\n\\nTool use (function calling) gives a model access to tools that can help it generate responses for messages that you send to the model. For example, you might have a chat application that lets users find out the most popular song played on a radio station. To answer a request for the most popular song, a model needs a tool that can query and return the song information. Amazon Bedrock Studio\\n\\nExplore Amazon Bedrock Studio (in preview), an SSO-enabled web interface that provides a way for developers across your organization to experiment with LLMs and other FMs, collaborate on projects, and iterate on generative AI applications. It offers a rapid prototyping environment and streamlines access to multiple foundation models (FMs) and developer tools in Amazon Bedrock. It also supports Amazon Bedrock Knowledge Bases and Amazon Bedrock Guardrails. Prompt management\\n\\nUse Amazon Bedrock to create and save your own prompts using Prompt management, so that you can save time by applying the same prompt to different workflows. When you create a prompt, you can select a model to run inference on it and modify the inference parameters to use. You can include variables in the prompt so that you can adjust the prompt for different use case. Understand 6\\n\\nPrompt flows\\n\\nPrompt flows for Amazon Bedrock offers the ability for you to use supported FMs to build workflows by linking prompts, foundational models, and other AWS services to create comprehensive solutions. With prompt flows, you can quickly build complex generative AI workflows using a visual builder. You can integrate with Amazon Bedrock offerings such as FMs, knowledge bases, and other AWS services such as AWS Lambda by transferring data between them. You can also deploy immutable workflows to move from testing to production in few clicks. Amazon SageMaker AI — Build custom models and control the full ML lifecycle, from data preparation to model deployment and monitoring\\n\\nWith Amazon SageMaker AI, you can build, train, and deploy machine learning models, including FMs, at scale. Consider this option when you have use cases that can benefit from extensive training, fine-tuning, and customization of foundation models. It also streamlines the sometimes- challenging task of evaluating which FM is the best fit for your use case.\\n\\nAmazon SageMaker AI also provides infrastructure and purpose-built tools for use throughout the ML lifecycle, including integrated development environments (IDEs), distributed training infrastructure, governance tools, machine learning operations (MLOps) tools, inference options and recommendations, and model evaluation.\\n\\nExplore key features of Amazon SageMaker AI that may help you determine when to use it:\\n\\nSageMaker AI JumpStart\\n\\nAmazon SageMaker AI JumpStart is an ML hub that provides access to publicly available foundation models. Those models include Mistral, Llama 3, CodeLlama, and Falcon 2. They can be customized with advanced fine-tuning and deployment techniques such as Parameter Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA). This following screenshot shows some of the available models in SageMaker AI JumpStart within the AWS Management Console. Understand 7\\n\\nSageMaker AI Clarify\\n\\nAmazon SageMaker AI Clarify addresses the all-important decision of which foundation model to use. Use SageMaker AI Clarify to create model evaluation jobs. A model evaluation job evaluates and compares model quality and responsibility metrics for text-based foundation models from JumpStart. Model evaluation jobs also support the use of JumpStart models that have already been deployed to an endpoint. SageMaker AI Canvas\\n\\nWith Amazon SageMaker AI Canvas, you can use machine learning to generate predictions without writing any code. You can also use Amazon SageMaker AI Canvas in collaboration with Amazon Bedrock to fine-tune and deploy language models. This blog post describes how you can use them to optimize customer interaction by working with your own datasets (such as your product FAQs) in Amazon Bedrock and Amazon SageMaker AI JumpStart. The following diagram, from this blog post, demonstrates how SageMaker AI Canvas and Amazon Bedrock can be used together to fine-tune and deploy language models. Understand 8\\n\\nSageMaker AI Studio\\n\\nAmazon SageMaker AI Studio is a web-based experience for running ML workflows. Studio offers a suite of integrated development environments (IDEs). These include Code Editor, based on Code-OSS, Visual Studio Code - Open Source, a new JupyterLab application, RStudio, and Amazon SageMaker Studio Classic. For more information, see Applications supported in Amazon SageMaker AI Studio. The web-based UI in Studio provides access to all SageMaker AI resources, including jobs and endpoints, in one interface. ML practitioners can also choose their preferred IDE to accelerate ML development. A data scientist can use JupyterLab to explore data and tune models. In addition, a machine learning operations (MLOps) engineer can use Code Editor with the pipelines tool in Studio to deploy and monitor models in production. SageMaker AI Studio includes generative AI assistance, powered by Amazon Q Developer right within your JupyterLab Integrated Development Environment (IDE). With Q Developer, you can access expert guidance on SageMaker AI features, code generation, and troubleshooting. Infrastructure for FM training and inference\\n\\nAWS offers specialized, accelerated hardware for high performance ML training and inference.\\n\\nAmazon EC2 P5 instances are equipped with NVIDIA H100 Tensor Core GPUs, which are well- suited for both training and inference tasks in machine learning.\\n\\nUnderstand 9\\n\\nAmazon EC2 G5 instances feature up to 8 NVIDIA A10G Tensor Core GPUs, and second generation AMD EPYC processors, for a wide range of graphics-intensive and machine learning use cases.\\n\\nAWS Trainium is the second-generation ML accelerator that AWS has purpose-built for deep learning (DL) training of 100B+ parameter models.\\n\\nAWS Inferentia2-based Amazon EC2 Inf2 instances are designed to deliver high performance at the lowest cost in Amazon EC2 for your DL and generative AI inference applications.\\n\\nConsider........................................................................................................................................................\\n\\nAfter you've decided on a generative AI service, choose the foundation model (FM) that gives you the best results for your use case.\\n\\nAmazon Bedrock has a model evaluation capability that can assist in evaluating, comparing, and selecting the best FMs for your use case. For more details on this capability, see Amazon Bedrock model evaluation is now generally available on the AWS News Blog.\\n\\nHere are some critical factors to consider when choosing an appropriate FM for your use case:\\n\\nModality\\n\\nIdentify use cases/modality What it is: Modality refers to the type of data the model processes: text, images (vision), or embeddings. Why it matters: The choice of modality should align with the data that you're working with. For example, if your project involves processing natural language, a text-based model like Claude, Llama 3.1, or Titan Text G1 is suitable. If you want to create embeddings, then you might use a model like Titan Embeddings G1. Similarly, for image-related tasks, models such as Stable Diffusion XL, and Titan Image Generator v2, are more appropriate. Your use case might also involve considering your data source and the support for data source connectors, such as those provided in Amazon Q Business. Model size\\n\\nModel Size What it is: This criterion refers to the number of parameters in a model. A parameter is a configuration variable that is internal to the model. Its values can be estimated (trained) Consider 10\\n\\nduring the training phase from the given training data. Parameters are crucial as they directly define the model's capability to learn from data. Large models often have more than 50 billion parameters. Why it matters: The number of parameters is a key indicator of the model's complexity. More parameters mean that the model can capture more intricate patterns and nuances in the data, which generally leads to better performance. However, these models are not only expensive to train, but also require more computational resources to operate. Inference latency\\n\\nInference latency What it is: Inference speed , or latency , is the time it takes for a model to process input (often measured in tokens) and return an output. This processing time is crucial when the model's responses are part of an interactive system, like an AWS Chatbot. Why it matters: Quick response times are essential for real-time applications such as interactive chatbots or instant translation services. These applications depend on the model's ability to process and respond to prompts rapidly to maintain a smooth user experience. Although larger FMs typically offer more detailed and accurate responses, their complex architectures can lead to slower inference speeds. This slower processing might frustrate users expecting immediate interaction. To address this challenge, you can choose models optimized for quicker responses, even if it means compromising somewhat on the responses' depth or accuracy. Context window\\n\\nMaximizing context window What it is: A large language model's context window is the amount of text (in tokens) that the model can consider at any one time when generating responses. Why it matters: Larger context windows enable the model to remember and process more information in a single run. This ability is particularly valuable in complex tasks such as understanding long documents, engaging in detailed conversations, or generating contextually accurate text over larger spans. For example, in a conversation, a model with a larger context window can remember more of the earlier dialogue, and provide responses that are more relevant to the entire conversation. Consider 11\\n\\nThis leads to a more natural and satisfying user experience, as the model can maintain the thread of discussion without losing context. Pricing\\n\\nPricing considerations What it is: The cost of using an FM is influenced by the model's complexity and the model provider’s pricing structure. Why it matters: Deploying high-performance models often comes with high costs due to increased computational needs. While these models provide advanced capabilities, their operational expenses can be high, particularly for startups or smaller projects on tight budgets. Smaller, less resource-intensive models offer a more budget-friendly option without significantly compromising performance. Weigh the model’s cost against its benefits to ensure it fits within your project's financial constraints and gets you the best value for your investment. Fine-tuning\\n\\nFine-tuning and continuous pre-training capability What it is: Fine-tuning is a specialized training process in which a pre-trained model that has been trained on a large, generic dataset is further trained (or fine-tuned) on a smaller, specific dataset. This process adapts the model to particularities of the new data, improving its performance on related tasks. Continuous pre-training, on the other hand, involves extending the initial pre-training phase with additional training on new, emerging data that wasn't part of the original training set, helping the model stay relevant as data evolves. You can also use Retrieval Augmented Generation (RAG) to retrieve data from outside an FM and augment your prompts by adding the relevant retrieved data in context. Why it matters: With fine-tuning, you can increase model accuracy by providing your own task- specific labeled training dataset and further specialize your FMs. With continued pre-training, you can train models using your own unlabeled data in a secure and managed environment. Continuous pre-training helps models become more domain-specific by accumulating more robust knowledge and adaptability beyond their original training. Data quality\\n\\nData quality Data quality is a critical factor in the success of a generative AI application. Consider the following quality factors: Consider 12\\n\\nRelevance: Ensure that the data you use for training your generative AI model is relevant to your application. Irrelevant or noisy data can lead to poor model performance.\\n\\nAccuracy: The data should be accurate and free from errors. Inaccurate data can mislead your model and result in incorrect outputs.\\n\\nConsistency: Maintain consistency in your data. Inconsistencies in the data can confuse the model and hinder its ability to learn patterns.\\n\\nBias and fairness: Be aware of biases in your data, as they can lead to biased model outputs. Take steps to mitigate bias and help ensure fairness in your generative AI system.\\n\\nAnnotation and labeling: If your application requires labeled data, verify that the annotations or labels are of high quality and created by experts.\\n\\nData preprocessing: Prepare your data by cleaning and preprocessing it. This might involve text tokenization, image resizing, or other data-specific transformations to make it suitable for training.\\n\\nData quantity\\n\\nData quantity Quantity along with quality goes hand in hand. Consider the following quantity factors: - Sufficient data: In most cases, more data is better. Larger datasets allow your model to learn a wider range of patterns and generalize better. However, the required amount of data can vary depending on the complexity of your application. - Data augmentation: If you have limitations on the quantity of available data, consider data augmentation techniques. These techniques involve generating additional training examples by applying transformations to existing data. For example, you can rotate, crop, or flip images or paraphrase text to create more training samples. - Balancing data: Ensure that your dataset is balanced, especially if your generative AI application is expected to produce outputs with equal representation across different categories or classes. Imbalanced datasets can lead to biased model outputs. - Transfer learning: For certain applications, you can use pre-trained models. With transfer learning, you can use models that were trained on massive datasets and fine-tune them with your specific data, often requiring less data for fine-tuning.\\n\\nConsider 13\\n\\nIt's also important to continuously monitor and update your dataset as your generative AI applications evolve and as new data becomes available. Quality of response\\n\\nQuality of response What it is: The most essential criterion is the quality of response. This is where you evaluate the output of a model based on several quality metrics, including accuracy, relevance, toxicity, fairness, and robustness against adversarial attacks. - Accuracy measures how often the model's responses are correct (and you would typically measure this against a pre-configured standard or baseline). - Relevance assesses how appropriate the responses are to the context or question posed. - Toxicity checks for harmful biases or inappropriate content in the model's outputs. - Fairness evaluates whether the model's responses are unbiased across different groups. - Robustness indicates how well the model can handle intentionally misleading or malicious inputs designed to confuse it.\\n\\nWhy it matters: The reliability and safety of model outputs are paramount, especially in applications that interact directly with users or make automated decisions that can affect people's lives. High-quality responses ensure user trust and satisfaction, reducing the risk of miscommunication and enhancing the overall user experience, thus earning the trust of your customers.\\n\\nChoose..........................................................................................................................................................\\n\\nGenerative AI category What is it optimized for? Generative AI services Amazon Q Generating code and providing responses to questions across business data by connecting to enterprise data repositories to summarize the data logically, Amazon Q Business Amazon Q Developer Choose 14\\n\\nGenerative AI category What is it optimized for? Generative AI services analyze trends, and engage in dialogue about the data. Amazon Bedrock Offering a choice of foundatio n models, customizing them with your own data, and building generative AI applications with the builder tools that Amazon Bedrock offers. Amazon Bedrock Amazon Bedrock Studio Amazon SageMaker AI Building, training, and deploying machine learning models, including foundation models, at scale. Amazon SageMaker AI Amazon FMs Providing models that support a variety of multi- modal use cases such as text, image, and embeddings. Amazon Titan Infrastructure for FM training and inference Offering services that maximize the price performan ce benefits in FM training and inference. AWS Trainium AWS Inferentia\\n\\nUse.................................................................................................................................................................\\n\\nNow that we've covered the criteria you need to apply in choosing an AWS generative AI service, you can select which services are optimized for your needs and explore how you might get started using each of them.\\n\\nAmazon Q\\n\\nGet started with Amazon Q\\n\\nUse 15\\n\\nReview your options for getting started with Amazon Q Business and Amazon Q Developer in either the AWS Management Console or the IDE. Explore the guide - Work with Amazon Q\\n\\nUse the Amazon Q Business and Amazon Q Developer User Guides, as well as the Amazon Q Business API Reference, to learn how you can tailor Amazon Q to your business needs. Learn how Amazon Q Business and Amazon Q Developer can help you understand, build, extend, and operate applications and workloads on AWS. Explore the guides - Learn Amazon Q\\n\\nTake this short, introductory AWS Skill Builder course to get a high-level overview of Amazon Q (requires registration). Start the course Amazon Q Business\\n\\nWhat is Amazon Q Business?\\n\\nGet an overview of Amazon Q Business, with explanations of what it is, how it works, and how to get started using it. Explore the guide - Create a sample Amazon Q Business application\\n\\nLearn how to create your first Amazon Q Business application in either the AWS Management Console or using the command line interface (CLI). Explore the guide - Combine Amazon Q Business and AWS IAM Identity Center to build generative AI apps\\n\\nUse 16\\n\\nBuild private and secure enterprise generative AI apps with Amazon Q Business and AWS IAM Identity Center. Read the blog post Amazon Q Developer\\n\\nWhat is Amazon Q Developer?\\n\\nGet an overview of Amazon Q Developer, with explanations of what it is, how it works, and how to get started using it. Explore the guide - Get started with Amazon Q Developer\\n\\nRead this blog post to explore some key tasks that you can accomplish with Amazon Q Developer. Read the blog post - Working with Amazon Q Developer\\n\\nUse the Amazon Q Developer Center for fast access to key Amazon Q Developer articles, blog posts, videos, and tips. Explore the Amazon Q Developer Center Amazon Bedrock\\n\\nWhat is Amazon Bedrock?\\n\\nLearn how to use this fully managed service to make foundation models (FMs) from Amazon and third parties available for your use through a unified API. Explore the guide - Frequently asked questions about Amazon Bedrock Use 17\\n\\nGet answers to the most commonly-asked questions about Amazon Bedrock, including how to use agents, security considerations, details on Amazon Bedrock software development kits (SDKs), Retrieval Augmented Generation (RAG), how to use model evaluation, and how billing works. Read the FAQs - Guidance for generating product descriptions with Amazon Bedrock\\n\\nLearn how to use Amazon Bedrock as part of a solution to automate your product review and approval process for an ecommerce marketplace or retail website. Explore the solution Amazon Bedrock Studio\\n\\nWhat is Amazon Bedrock Studio?\\n\\nLearn how you can use this web application to prototype apps that use Amazon Bedrock models and features, without having to set up and use a developer environment. Explore the guide - Build generative AI applications with Amazon Bedrock Studio (preview)\\n\\nThis blog explains how you can build applications using a wide array of top-performing models, as well as how to evaluate and share your generative AI apps within Amazon Bedrock Studio. Read the blog post - Building an app with Amazon Bedrock Studio\\n\\nUse the Build mode in Amazon Bedrock Studio to create prototype apps that use Amazon Bedrock models and features. You can also use the Build mode to try experiments not supported in the Explore mode playground, such as setting inference parameters. Explore the guide Use 18\\n\\nAmazon SageMaker AI\\n\\nWhat is Amazon SageMaker AI?\\n\\nLearn how you can use this fully managed machine learning (ML) service to build, train, and deploy ML models into a production-ready hosted environment. Explore the guide - Get started with Amazon SageMaker AI\\n\\nLearn how to join an Amazon SageMaker AI domain, giving you access to Amazon SageMaker AI Studio and RStudio on SageMaker AI. Explore the guide - Get started with Amazon SageMaker AI JumpStart\\n\\nExplore SageMaker AI JumpStart solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker AI. Explore the guide Amazon Titan\\n\\nAmazon Titan in Amazon Bedrock overview\\n\\nGet an overview of Amazon Titan foundation models (FMs) to support your use cases. Explore the guide - Cost-effective document classification using the Amazon Titan Multimodal Embeddings Model\\n\\nLearn how you can use this model to categorize and extract insights from high volumes of documents of different formats. This blog explores how you can use it to help determine the next set of actions to take, depending on the type of document. Read the blog post Use 19\\n\\nBuild generative AI applications with Amazon Titan Text Premier, Amazon Bedrock, and AWS CDK\\n\\nExplore building and deploying two sample applications powered by Amazon Titan Text Premier in this blog post. Read the blog post AWS Trainium\\n\\nOverview of AWS Trainium\\n\\nLearn about AWS Trainium, the second-generation machine learning (ML) accelerator that AWS purpose built for deep learning training of 100B+ parameter models. Each Amazon EC2 Trn1 instance deploys up to 16 AWS Trainium accelerators to deliver a high-performance, low-cost solution for deep learning (DL) training in the cloud. Explore the guide - Recommended Trainium Instances\\n\\nExplore how AWS Trainium instances are designed to provide high performance and cost efficiency for deep learning model inference workloads. Explore the guide - Scaling distributed training with AWS Trainium and Amazon EKS\\n\\nIf you're deploying your deep learning (DL) workloads using Amazon Elastic Kubernetes Service (Amazon EKS), learn how you can benefit from the general availability of Amazon EC2 Trn1 instances powered by AWS Trainium—a purpose-built ML accelerator optimized to provide a high-performance, cost-effective, and massively scalable platform for training DL models in the cloud. Read the blog post Use 20\\n\\nAWS Inferentia\\n\\nOverview of AWS Inferentia\\n\\nUnderstand how AWS designs accelerators to deliver high performance at the lowest cost for your deep learning (DL) inference applications. Explore the guide - AWS Inferentia2 builds on AWS Inferentia1 by delivering 4x higher throughput and 10x lower latency\\n\\nUnderstand what AWS Inferentia2 is optimized for and how it was designed to deliver higher performance, while lowering the cost of LLMs and generative AI inference. Read the blog post - Machine learning inference using AWS Inferentia\\n\\nLearn how to create an Amazon EKS cluster with nodes running Amazon EC2 Inf1 instances and optionally deploy a sample application. Amazon EC2 Inf1 instances are powered by AWS Inferentia chips, which are custom built by AWS to provide high-performance and low-cost inference in the cloud. Explore the guide\\n\\nExplore..........................................................................................................................................................\\n\\nArchitecture diagrams\\n\\nThese reference architecture diagrams show examples of AWS AI and ML services in use. Explore architecture diagrams - Whitepapers\\n\\nExplore whitepapers to help you get started and learn best practices in choosing and using AI and ML services. Explore 21\\n\\nExplore whitepapers - AWS solutions\\n\\nExplore vetted solutions and architectural guidance for common use cases for AI and ML services. Explore solutions\\n\\nResources......................................................................................................................................................\\n\\nPublic foundation models\\n\\nSupported foundation models are updated on a regular basis, and currently include:\\n\\nAnthropic Claude\\n\\nCohere Command & Embed\\n\\nAI21 Labs Jurassic\\n\\nMeta Llama\\n\\nMistral AI\\n\\nStable Diffusion XL\\n\\nAmazon Titan\\n\\nUse Amazon Bedrock and Amazon SageMaker AI to experiment with a variety of foundation models, and privately customize them with your data. To explore generative AI quickly, you also have the option of using PartyRock, an Amazon Bedrock Playground. PartyRock is a generative AI app building playground with which you can experiment hands-on with prompt engineering.\\n\\nAssociated blog posts\\n\\nBuild private and secure enterprise generative AI apps with Amazon Q Business and IAM Identity Center\\n\\nAmazon Q Developer, now generally available, includes previews of new capabilities to reimagine developer experience\\n\\nChat about your AWS account resources with Amazon Q Developer\\n\\nBuild enterprise-grade applications with natural language using AWS App Studio (preview)\\n\\nResources 22\\n\\nAmazon Bedrock model evaluation is now generally available\\n\\nBuild generative AI applications with Amazon Bedrock Studio (preview)\\n\\nFine-tune and deploy language models with Amazon SageMaker AI\\n\\nResources 23\\n\\nDocument history..........................................................................................................................\\n\\nThe following table describes the important changes to this decision guide. For notifications about updates to this guide, you can subscribe to an RSS feed.\\n\\nChange Description Date Updated content Updated Amazon Bedrock feature names, including Amazon Bedrock Agents, Amazon Bedrock Guardrails, Amazon Bedrock Knowledge Bases, and Amazon Bedrock Custom Model Import. August 28, 2024 Updated content Minor updates to improve readability. August 16, 2024 Updated content Updates for newly released features of Amazon Q, Amazon SageMaker AI, and Amazon Bedrock. July 18, 2024 Initial release Initial release of the decision guide. July 9, 2024 24\")]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chuncks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143,\n",
       " Document(metadata={'source': 'data\\\\bedrock_or_sagemaker.md', 'start_index': 0}, page_content=\"AWS Decision guide\\n\\nAmazon Bedrock or Amazon SageMaker AI?\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon Bedrock or Amazon SageMaker AI?: AWS Decision guide\\n\\nCopyright © 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.\\n\\nTable of Contents\"))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chuncks), chuncks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH = Path(\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    chuncks, OpenAIEmbeddings(), persist_directory=str(CHROMA_DB_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x21f27a0fc10>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "db  = Chroma(persist_directory=str(CHROMA_DB_PATH),\n",
    "             embedding_function=embedding_function)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"\"\"How to use AWS Batch to run batch computing workloads in the AWS Cloud.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results  = db.similarity_search_with_relevance_scores(query_text, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'data\\\\containers-on-aws-how-to-choose.md', 'start_index': 21247}, page_content='Explore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\\n\\nWhat is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\\n\\nUse these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\\n\\nTools and services with container support......................................................................................\\n\\nAWS Copilot'),\n",
       "  0.8591639770874674),\n",
       " (Document(metadata={'source': 'data\\\\containers-on-aws-how-to-choose.md', 'start_index': 21247}, page_content='Explore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\\n\\nWhat is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\\n\\nUse these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\\n\\nTools and services with container support......................................................................................\\n\\nAWS Copilot'),\n",
       "  0.8591040709789054),\n",
       " (Document(metadata={'source': 'data\\\\containers-on-aws-how-to-choose.md', 'start_index': 8320}, page_content='AWS Batch: AWS Batch is a fully managed service with which you can run batch computing workloads on AWS. It dynamically provisions the optimal compute resources based on the volume and specific resource requirements of the batch jobs that you submit. It automatically handles job scheduling, resource provisioning, and scaling based on the workload requirements.\\n\\nAmazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker container registry with which you can store, manage, and deploy Docker container images. It is designed to provide secure and scalable storage for your container images and simplify provisioning containers with the desired images.\\n\\nNote AWS provides a variety of ways to deploy and run containers. One of the first considerations is your preference for either a serverless operational model or a Kubernetes operation model. In practice, most customers use both to varying degrees. Understand 4'),\n",
       "  0.8170703495095305)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\n",
      "\n",
      "What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\n",
      "\n",
      "Use these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\n",
      "\n",
      "Tools and services with container support......................................................................................\n",
      "\n",
      "AWS Copilot\n",
      "Explore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\n",
      "\n",
      "What is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\n",
      "\n",
      "Use these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\n",
      "\n",
      "Tools and services with container support......................................................................................\n",
      "\n",
      "AWS Copilot\n",
      "AWS Batch: AWS Batch is a fully managed service with which you can run batch computing workloads on AWS. It dynamically provisions the optimal compute resources based on the volume and specific resource requirements of the batch jobs that you submit. It automatically handles job scheduling, resource provisioning, and scaling based on the workload requirements.\n",
      "\n",
      "Amazon Elastic Container Registry (Amazon ECR): Amazon ECR is a fully managed Docker container registry with which you can store, manage, and deploy Docker container images. It is designed to provide secure and scalable storage for your container images and simplify provisioning containers with the desired images.\n",
      "\n",
      "Note AWS provides a variety of ways to deploy and run containers. One of the first considerations is your preference for either a serverless operational model or a Kubernetes operation model. In practice, most customers use both to varying degrees. Understand 4\n"
     ]
    }
   ],
   "source": [
    "for content in results:\n",
    "    print(content[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(\"No relevant documents found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = results[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\\n\\nWhat is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\\n\\nUse these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\\n\\nTools and services with container support......................................................................................\\n\\nAWS Copilot'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {query_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question based only on the following context:\\n\\nExplore the guide Amazon Lightsail resource center Explore Lightsail tutorials, videos, and links to core concept documentation. Visit the resource center AWS Batch\\n\\nWhat is AWS Batch? Learn how to use AWS Batch to run batch computing workloads in the AWS Cloud. Explore the guide Best practices for AWS Batch Consider this guidance on how to run and optimize your workloads when using AWS Batch. Explore the guide AWS Batch workshops center Vertical solutions 15\\n\\nUse these workshops, organized in a progressive manner from beginner to advanced, to explore and learn AWS Batch. Explore the workshops\\n\\nTools and services with container support......................................................................................\\n\\nAWS Copilot\\n\\n---\\n\\nAnswer the question based on the above context: How to use AWS Batch to run batch computing workloads in the AWS Cloud.\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "response_text = model.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\containers-on-aws-how-to-choose.md',\n",
       " 'data\\\\containers-on-aws-how-to-choose.md',\n",
       " 'data\\\\containers-on-aws-how-to-choose.md']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Response: To use AWS Batch to run batch computing workloads in the AWS Cloud, you can explore the guide and best practices provided in the AWS Batch resource center. Additionally, you can participate in workshops to gain hands-on experience and learn more about utilizing AWS Batch effectively.\\nSources: ['data\\\\\\\\containers-on-aws-how-to-choose.md', 'data\\\\\\\\containers-on-aws-how-to-choose.md', 'data\\\\\\\\containers-on-aws-how-to-choose.md']\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "formatted_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
